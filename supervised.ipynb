{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd329802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20000\n",
      "Features used: ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'key', 'mode', 'count', 'duration_min', 'is_instrumental', 'is_acoustic', 'main_genre_encoded']\n",
      "Train shape: (16000, 17) Test shape: (4000, 17)\n",
      "\n",
      "Baseline model (mean popularity):\n",
      "RMSE: 22.495\n",
      "MAE : 19.123\n"
     ]
    }
   ],
   "source": [
    "# Notebook 2: Supervised learning on popularity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"data_cleaned.csv\")\n",
    "\n",
    "# Features and target\n",
    "feature_cols = [\n",
    "    \"acousticness\", \"danceability\", \"duration_ms\", \"energy\",\n",
    "    \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\",\n",
    "    \"tempo\", \"valence\", \"key\", \"mode\", \"count\",\n",
    "    \"duration_min\", \"is_instrumental\", \"is_acoustic\",\n",
    "    \"main_genre_encoded\"\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "target_col = \"popularity\"\n",
    "\n",
    "data = df.dropna(subset=[target_col] + feature_cols).copy()\n",
    "df_sample = data.sample(20000, random_state=42)\n",
    "\n",
    "X = df_sample[feature_cols]\n",
    "y = df_sample[target_col]\n",
    "\n",
    "print(\"Number of samples:\", len(X))\n",
    "print(\"Features used:\", feature_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# Baseline: predict mean popularity\n",
    "y_mean = np.full_like(y_test, y_train.mean(), dtype=float)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_mean))\n",
    "baseline_mae = mean_absolute_error(y_test, y_mean)\n",
    "print(\"\\nBaseline model (mean popularity):\")\n",
    "print(f\"RMSE: {baseline_rmse:.3f}\")\n",
    "print(f\"MAE : {baseline_mae:.3f}\")\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"KNN_10\": KNeighborsRegressor(n_neighbors=10)\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    rows.append([name, rmse, mae, r2])\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=[\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n",
    "print(\"\\nModel comparison:\")\n",
    "print(results_df.sort_values(\"RMSE\"))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=results_df, x=\"Model\", y=\"RMSE\")\n",
    "plt.title(\"RMSE by model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross validation on Random Forest\n",
    "rf_base = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "cv_scores = cross_val_score(rf_base, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"\\nRandom Forest CV R2:\", cv_scores)\n",
    "print(\"Mean CV R2:\", cv_scores.mean())\n",
    "\n",
    "# Simple hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [None, 20],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"r2\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest params:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Best CV R2:\", grid.best_score_)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTuned Random Forest on test:\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE : {mae:.3f}\")\n",
    "print(f\"R2  : {r2:.3f}\")\n",
    "print(f\"RMSE gain vs baseline: {baseline_rmse - rmse:.3f}\")\n",
    "\n",
    "# Predicted vs actual\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([0, 100], [0, 100], \"r-\")\n",
    "plt.xlabel(\"Actual popularity\")\n",
    "plt.ylabel(\"Predicted popularity\")\n",
    "plt.title(\"Predicted vs actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(residuals, bins=50, kde=True)\n",
    "plt.title(\"Residuals\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "plt.axhline(0, color=\"r\")\n",
    "plt.xlabel(\"Predicted popularity\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residuals vs predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Permutation importance\n",
    "perm = permutation_importance(\n",
    "    best_rf, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "importances = pd.Series(perm.importances_mean, index=feature_cols).sort_values()\n",
    "\n",
    "print(\"\\nPermutation feature importance:\")\n",
    "print(importances.sort_values(ascending=False))\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "importances.tail(15).plot(kind=\"barh\")\n",
    "plt.title(\"Feature importance (top 15)\")\n",
    "plt.xlabel(\"Mean decrease in R2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
